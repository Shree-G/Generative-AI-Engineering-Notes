{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73c3f1d8",
   "metadata": {},
   "source": [
    "Tokenization and the building of a vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df7d0e8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "lines = [\"IBM taught me tokenization\", \n",
    "         \"Special tokenizers are ready and they will blow your mind\", \n",
    "         \"just saying hi!\"]\n",
    "\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "tokenizer_en = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "tokens = []\n",
    "max_length = 0\n",
    "\n",
    "for line in lines:\n",
    "    tokenized_line = tokenizer_en(line)\n",
    "    tokenized_line = ['<bos>'] + tokenized_line + ['<eos>']\n",
    "    tokens.append(tokenized_line)\n",
    "    max_length = max(max_length, len(tokenized_line))\n",
    "\n",
    "for i in range(len(tokens)):\n",
    "    tokens[i] = tokens[i] + ['<pad>'] * (max_length - len(tokens[i]))\n",
    "\n",
    "print(\"Lines after adding special tokens:\\n\", tokens)\n",
    "\n",
    "# Build vocabulary without unk_init\n",
    "vocab = build_vocab_from_iterator(tokens, specials=['<unk>'])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "# Vocabulary and Token Ids\n",
    "print(\"Vocabulary:\", vocab.get_itos())\n",
    "print(\"Token IDs for 'tokenization':\", vocab.get_stoi())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74412a02",
   "metadata": {},
   "source": [
    "output:\n",
    "\n",
    "Lines after adding special tokens:\n",
    " [['<bos>', 'IBM', 'taught', 'me', 'tokenization', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'], ['<bos>', 'Special', 'tokenizers', 'are', 'ready', 'and', 'they', 'will', 'blow', 'your', 'mind', '<eos>'], ['<bos>', 'just', 'saying', 'hi', '!', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']]\n",
    "Vocabulary: ['<unk>', '<pad>', '<bos>', '<eos>', '!', 'IBM', 'Special', 'and', 'are', 'blow', 'hi', 'just', 'me', 'mind', 'ready', 'saying', 'taught', 'they', 'tokenization', 'tokenizers', 'will', 'your']\n",
    "Token IDs for 'tokenization': {'will': 20, 'tokenizers': 19, 'tokenization': 18, 'taught': 16, 'your': 21, 'saying': 15, '<unk>': 0, 'and': 7, 'hi': 10, '<pad>': 1, '<bos>': 2, 'they': 17, '<eos>': 3, '!': 4, 'ready': 14, 'IBM': 5, 'are': 8, 'Special': 6, 'mind': 13, 'me': 12, 'blow': 9, 'just': 11}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a801a08f",
   "metadata": {},
   "source": [
    "Let's break down the output:\n",
    "\n",
    "Special Tokens:\n",
    "Token: \"<unk>\", Index: 0: <unk> stands for \"unknown\" and represents words that were not seen during vocabulary building, usually during inference on new text.\n",
    "Token: \"<pad>\", Index: 1: <pad> is a \"padding\" token used to make sequences of words the same length when batching them together.\n",
    "Token: \"<bos>\", Index: 2: <bos> is an acronym for \"beginning of sequence\" and is used to denote the start of a text sequence.\n",
    "Token: \"<eos>\", Index: 3: <eos> is an acronym for \"end of sequence\" and is used to denote the end of a text sequence.\n",
    "Word Tokens: The rest of the tokens are words or punctuation extracted from the provided sentences, each assigned a unique index:\n",
    "Token: \"IBM\", Index: 5\n",
    "Token: \"taught\", Index: 16\n",
    "Token: \"me\", Index: 12 ... and so on.\n",
    "Vocabulary: It denotes the total number of tokens in the sentences upon which vocabulary is built.\n",
    "\n",
    "Token IDs for 'tokenization': It represents the token IDs assigned in the vocab where a number represents its presence in the sentence."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
