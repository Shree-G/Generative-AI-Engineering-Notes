# Intro
- Agent anatomy can be broken down into 3 main parts
	- Model (the brain)
	- Tools (the hands)
	- Orchestration (the conductor)
- General flow of agent:
	- the model reasons about which tool is needed for the next step in the process, and the orchestration layer actually calls that tool
	- the result (the observation) from that tool call is then fed back into the model for future thought cycles

- Model (LLM)
	- key function: managing the context window
	- constantly deciding what's important
	- information comes from the mission, memory, what the tools just did
	- the model curates the input context window of the LM
- Tools
	- the connection to the outside world
	- APIs, databases, vector stores, code functions
- Orchestration
	- manages the operational loop
	- keeps track of memory or state
	- manages the reasoning strategy (like chain of thought or ReAct)
	- ReAct: thinking, acting, observing, then cycle
- Deployment
	- hosting the agent on a secure, scalable server
	- integrating it with tools to make it production ready like monitoring, logging and management
	- once deployed, can be accessed by users through a frontend or via an Agent to Agent (A2A) API

- if traditional development is like "bricklaying", agentic development is like "directing" or "conducting"
- an LM's greatest strength being it's flexibility to do *anything* is often an agentic system's greatest weakness, because you are forcing it to do one specific thing
- Agents are software which manage the inputs of LMs to get work done
- In essence, an agent is a system dedicated to the art of **what to include in the context window**. The context can include system instructions, user input, session history, long term memories, grounding knowledge from authoritative sources, tools to be used, results of tools already invoked, etc.
- The sophisticated management of the model's attention allows its reasoning capabilities to problem solve for novel circumstances and accomplish objectives

## The Agentic Problem-Solving Process

What are the specific steps an Agent takes to accomplish it's mission?

1. Get the Mission
	- can be provided by user prompt
	- can be provided through automated trigger
2. Scan the Scene
	- this step involves gathering context from the environment
	- more technically, this means the orchestration layer accessing its available resources
	- ex questions:
		- "What does the user's request
		say?", "What information is in my term memory? Did I already try to do this task? Did the
		user give me guidance last week?", "What can I access from my tools, like calendars,
		databases, or APIs?"
3. Think it Through
	- The model assess it's mission against the available tools and context and comes up with a plan of action
	- this plan of action is often a chain of reasoning involving sequential tool use
4. Take Action
	- Does the first concrete step of the plan (uses the first tool it needs to use)
5. Observe and Iterate
	- Observes the output of the tool use (action) and adds that output to the agent's context or memory
	- Uses that new context to go back to step 3 and repeats steps 3-5 until the mission is complete

# A Taxonomy of Agentic Systems

### Level 0 - Core Reasoning System
- LM operating in isolation without any tools or ways to access realtime data
- lack of any real-time awareness, blind to any event or fact outside its training data

### Level 1 -  Connected Problem-Solver
- LM connected to tools that can interact with it's environment
- Using 5-step loop defined above, the agent can answer more realtime questions and do more complex tasks

### Level 2 - Strategic Problem-Solver
- can strategically plan complex, multi-part goals
- new skill for this level: **context engineering**: the ability for the agent to select, package and include the most relevant and necessary context for the reasoning of the next step
- context engineering is very important for agent accuracy

### Level 3 - Collaborative Multi-Agent System
- Basically a team of level 2 agents, however with specific roles and specialties
- this model directly reflects a human organization, with divisions of labor
- usually has an orchestrator agent in charge of other agents that delegates tasks

### Level 4 - Self Evolving System
- A level 3 system, but the ability to edit and create new tools and agents that can serve a purpose that the system sees a need for
- moves from using a fixed set of resources to actively expanding them
- this is the current frontier of agentic systems

# Core Agent Architecture

How do we actually build an agent? We split it up into three distinct parts: Model, Tools, and Orchestration
## Model
- should not pic the model with the highest benchmark score -not necessarily the best pick model for the task at hand
- important agentic fundamentals for models are **superior reasoning and reliable tool use**
- start by defining the business problem, then test models against metrics that directly map to that outcome
- the "best" model is the one that sits at the optimal intersection of speed, quality and price for your specific task
- if you have a team of specialists approach, then use heavy reasoning models for initial planning and complex reasoning, then switch to a faster more cost effective model for high volume tasks like classifying user intent or summarizing basic test
- importantly, the AI landscape is constantly evolving
	- building for this reality means building a CI/CD pipeline that continuously evaluates new models against key business metrics, so that you can de-risk and accelerate upgrades to your product

## Tools
- a tool interface is a three-part loop
	- defining what a tool can do for the LM to read and reason with
	- invoking the tool itself
	- observing the result

#### Main types of tools agents use:

Retrieving Information: Grounding in Reality
- RAG for unstructured data
- Natural Language to SQL (NL2SQL)
	- allows agent to query databases to answer analytic questions about the data at hand
- These types of tools reduce hallucinations because the LM can be grounded in the real data that it uses as context

Executing Actions: Changing the World
- wrapping existing APIs and code functions as tools
- for more dynamic tasks, an agent can potentially write and execute code on the fly
	- such as SQL query or a python script
- this also includes tool for human interaction, like Human in the Loop (HiTL) frameworks
	- can pause workflow and ask user for confirmation
	- request information from a user through a user interface

#### Connecting Tools to Agents
- there are standards for creating "tools" that agents can use
- an example of a standard for tool use is the OpenAPI specification, giving the agent a structured contract that describes a tool's purpose, its required parameters and its expected response
- For simpler, easier discovery and connection to tools, MCP is the standard
- A few models have native tool use, such as Gemini with google search

## Orchestration
- central nervous system that connects the model to the tools
- this is the engine that runs the "think, act, observe loop"

#### Core Design Choices
1. Determining agent's degree of autonomy
	- on one end: deterministic workflows that call an LM as a tool for a specific task
	- on the other end: the LM is in driver's seat, constantly making decisions and executing tasks to achieve a goal
2. Implementation method
	- no code builders: speed and accessibility
	- code first frameworks: control, customizability, and integration

- regardless of approach, the framework should be
	- open - should be able to use any model without vendor lock in
	- controllable - the reasoning of the LM is constrained by business logic
	- observable - a framework that generates traces, logs and exposes reasoning

#### Instruct with Domain Knowledge and Persona
- most powerful step is to instruct the agent with domain knowledge and a distinct persona
- this is usually accomplished through a system prompt, telling the LM what role it is playing in the agent architecture - agent's constitution
	- provide constraints, desired output schema, rules of engagement, a specific tone of voice, and explicit guidance on when and why it should use its tools
	- example scenarios in the instructions/system prompt is usually very effective as well

#### Augment with Context
- agent memory is nothing but the added context to the LM context window at runtime
- short term memory
	- agent's active scratchpad
	- maintaining the running history of the current conversation
	- tracks sequence of Action, Observation pairs from the current loop, providing the immediate context for the model to decide what to do next
	- can be implemented as state, artifacts, sessions or threads
- long term memory
	- persistence **across** sessions
	- architecturally this is almost always implemented as a RAG
	- the orchestrator gives the agent the ability to query it's own history, including user preferences or conversations it's had with a user a few weeks ago

#### Multi Agent Systems and Design Patterns
- Coordinator pattern
	- introduces a manager agent that analyzes a complex request, segments the primary task and routes each subtask to the appropriate specialist agent
	- coordinator then aggregates result into the required output schema
- Sequential pattern
	- acts as a digital assembly line where the output of one agent becomes the input of another agent
- Iterative Refinement pattern
	- creates a feedback loop with the generator agent to create content and the critic agent to evaluate it against quality standards
- Human in the loop pattern
	- creates deliberate pauses in the workflow to get human approval before an agent takes a significant action

## Agent Deployment and Services
- can deploy an agent specific deployment options like vertex AI Agent Engine
- or can deploy to a legacy traditional deployment runtimes in a docker container like cloud run or gke
- for agents in production, you should always have CI/CD pipelines and automated testing for your agents

#### Agent Ops
- traditional unit tests don't work for agentic systems, especially because you cannot simply assert that the `output ==  expected`
- because language is complicated, it usually requires another LM to evaluate "quality" of the output with the input query or prompt
- Agent Ops is this new field that deals with the challenges of building, deploying and governing AI agents

#### Measure what matters
- ask yourself what are the KPIs (Key Performance Indicators) that prove the agent is doing what it is supposed to be doing?
- these KPIs should be the likes of
	- goal completion rates
	- task latency
	- user satisfaction scores
	- operational cost per intraction
	- impact on business goals such as revenue, conversion or customer retention
- these KPIs will guide testing, and make you do metrics driven development

#### Using an LM as judge
- creation of evaluation datasets should be built with these metrics
	- the dataset must cover the full range of use cases that you expect the user to engage with
	- along with all edge cases that the user can do to break the system
- eval results should always be reviewed by a domain expert before being accepted as valid
- the curation and maintenance of these evaluations is becoming a key responsibility for product managers with the support of Domain experts

#### Metrics-Driven Development
- once you have an automated evaluation suite, testing is simple
- just run the new version against the same evaluation golden dataset and compare the scores to the existing production version
- these scores should include latency, cost, and task success rates
- **important**: for max safety, do A/B deployments to slowly roll out new versions and compare real-world production metrics along with simulation scores such as user feedback and error reporting

#### Debug with OpenTelemetry Traces
- with traces you can see the exact prompt sent to the model, the model's internal reasoning, the specific tool it chose to call (if any), the parameters it generated for that tool, and the raw data that came back as an observation
- useful for debugging not for metrics
- traces can be collected platforms like google cloud trace, or langsmith

#### Cherish Human Feedback
- when a user files a bug report, save that real world edge case and add it to your automated eval scenarios
- close the loop by capturing the feedback, replicating the issue, and converting that scenario into a specific permanent test case in your evaluation dataset
- this ensures you fix the but also vaccinates the system against that entire class of error ever happening again

## Agent Interoperability
- agents are not tools, there's a difference between connecting to agents vs connecting agents with data and APIs
#### Agents and Humans
- most common form of agent-human interaction is a chatbot
	- user types in a request and agent acts a backend service, processes it and returns a block of text
	- can provide structured data, like JSON, to power rich, dynamic front-end experiences
	- human in the loop interaction patterns can include intent refinement, goal expansion, confirmation and clarification requests
- computer use is where the LM takes control of a user interface often with human interaction and oversight
- LM can change the UI to meet the needs of the moment
	- done with tools which control UI (MCP UI, AG UI, A2UI)
- live, real-time, multimodal communication also exists, to enable bidirectional streaming of users speaking to an agent and interrupting it, just as they would in a natural conversation

#### Agents and Agents
- core challenge is twofold
	- discovery - how does my agent find other agents and know what they can do?
	- communication - what is the standard of communication that both agents know they are going to speak?
- Agent2Agent (A2A) protocol
	- allows any agent to publish a digital "business card" known as an Agent Card
	- agent card = JSON file that has agent's capabilities, its network endpoint, and security credentials required to interact with it
	- agents communicate using a task-oriented architecture
		- a client agent sends a task request to a server agent, which then provides **streaming updates** as it works on the problem over a long-running connection

#### Agents and Money
- giving agents access to money and purchasing power is risky and there are complex issues relating to authorization, authenticity and accountability
- Agents Payments Protocol (AP2)
	- has cryptographically-signed digital mandates that act as proof of user intent
	- creates an audit trail for every transaction
- x402
	- open internet payment protocol that uses the HTTP 402 "payment required" status code

## Securing a Agent
- if you give LM power to do things, then they might do rogue actions or expose sensitive information to third parties
- best practice is hybrid defense in depth approach
	- first layer - traditional deterministic guardrails 
		- set of hardcoded rules that act as security chokepoint
		- ex: block any purchase more than a $100 
	- second layer - reasoning-based defenses
		- training the model to be more resilient to attacks
		- employing smaller specialized guard models that act like security analysts
- hybrid model ensures that there are two different types of guardrails always protecting from malicious attacks

#### Agent Identity
- each agent on the platform must be issued a secure agent identity, that has permissions, service accounts, and more
- once an agent has a cryptographically verifiable identity, it can be granted agent specific privileges
	- using standards like SPIFFE
- ensures that even if a single agent is compromised, its hard for any one agent to deal lasting damage to the system
- policies can ensure that principals that only need access to a certain permission has access to it
	- often done at the API governance layer, along with governance supporting MCP and A2A services

#### Securing an Agent in practice
- clear definition of identities - user account (OAuth), service account (to run code), agent identity (to use delegated authority)
- establish policies to constrain access to services
- build guardrails into tools, models and sub-agents to enforce policies
	- ensures that a tool or sub-agent's own logic will refuse to execute an unsafe action
- ADK provides callbacks and plugins
	- callback allows you to inspect the parameters of a tool before it runs
	- plugins allow you to use LM to screen user inputs and agent outputs for prompt injections or harmful content
- Model Armor
	- dedicated service that does all this

## Scaling up from a single agent to fleet of agents
- lots more security concerns with **agent sprawl**, the complex interactions between many agents, tools and users might cause more security vulnerabilities

#### Security and Privacy
- possible attacks include
	- prompt injection
	- data poisoning to corrupt the information it uses for training or RAG
- ensure that an enterprise proprietary information is never used to train base models
- requires input and output filtering
- contractual protections like intellectual property indemnity for training data and generated output

#### Agent Governance: A Control Plane instead of Sprawl
- creating a single point of contact (gateway) between agents, tools and users creates observability of every interaction
- this control plane serves two functions
	- runtime policy enforcement
		- handles authentication and authorization
		- common logs, metrics and traces for every transaction
	- centralized governance
		- to enforce policies, the gateway needs a source truth
		- provided by central registry that has all agentic assets with necessary permissions
		- allows for security reviews as well as creation of fine-grained policies that dictate which business units can access which agents

#### Cost and Reliability
- spectrum of infrastructure options for scale-to-zero applications as well as provisioned throughput LM services for always on uptimes
- choose well!

## Self Evolving Agents
- agent performance will degrade over time because policies, technologies and data formats are constantly changing
- manually updating a large fleet of agents is uneconomical and slow
- design agents that can learn autonomously, improving quality on the job

#### How agents learn and evolve
- learning process is fueled by:
	- Runtime Experience
		- Session logs, traces, and memory that captures successes, failures, tool interactions and decision trajectories.
		- this includes HiTL feedback which provides authoritative corrections and guidance
	- External Signals
		- new external documents such as updated policies, public regulatory guidelines, or critiques from other agents
- this information is used to optimize agent future behavior
- instead of summarizing past interactions, advances systems create generalizable artifacts to guide future tasks
	- Q: does this mean that the agent will edit it's own artifacts when it gets new information from an external source?
- successful adaptation techniques include:
	- enhanced context engineering
		- system refines it's prompts, few-shot examples (?) and information it retrieves from. memory
		- by optimizing context, increases success rate
	- tool optimization and creation
		- when the agent identifies gaps in its capabilities, it creates new tools on the fly by itself
- this can take shape in the form of a specified **learning agent** that observes entire interactions and sees corrective feedback from the human expert. it then generalizes this feedback into a new reusable guideline for other agents' artifacts or rules
- also look into RLHF - emerging field with great practical use cases

#### Agent Gym
- a dedicated space where agents can run in an offline environment with advanced tooling and capabilities
- not in the execution path - so can use any new and untested tools, features, models, etc.
- simulation environment, so the agent can "exercise" on new data and learn
- can call advanced synthetic data generators which guide the simulation to be as real as possible and pressure test the agent
- open to new tools and capabilities, doesn't need to be as strict and can explore open protocols in a more advanced setting
- can consult with human domain experts to guide the next set of optimizations

# Day 2

# Tools and Tool Calling

## What do we mean by tool?
- Two base types of tools:
	- Allows a model to know something (usually real time)
	- Allows a model to do something (update a db, calling an API, executing other code, etc.)
## Types of Tools
- tool definition declares a contract between the model and the tool
	- includes clear name, parameters, and a natural language description that explains its purpose and how it should be used
- Three types of tools explained below
### Function Tools
- Tools defined as functions with descriptions and contracts about tool usage - description is provided as part of the request context
- These are external functions that the model can call as needed
### Built-in Tools
- Some LMs have built-in tools, where tool definition is given to model implicitly
- Gemini API can use google search, code execution, computer use, url context
### Agent Tools
- an agent can also be invoked as a tool
- allows the primary agent to maintain control over the interaction
- primary agent processes the sub-agent input and output as necessary
## Taxonomy of Agent Tools
Another way to categorize agent tools by their primary function:
- Information Retrieval
	- structured and unstructured data
- Action/Execution
	- real world operations such as sending emails, posting messages, code execution, controlling physical devices
- System / API Integration
	- connecting with existing software systems and APIs
- Human in the loop
	- seek approval for critical actions
## Best Practices for Tool Use
### Documentation is important

Tool documentation (name, description and attributes) are all passed down to the model as part of request context, so important that tools are well documented. Here are some general guidelines:
- use clear names for function name, parameter names, etc
- describe all input and output parameters
	- required type of input and output parameters
	- how the tool will use the parameter
- simplify parameter lists to keep them as short as possible
- clarify tool descriptions
	- purpose of tool
	- any details to call the tool effectively
	- avoid shorthand or technical jargon
- add targeted examples
	- can also dynamically retrieve examples related to the immediate task to minimize context bloat
- provide default values for key parameters
	- document and describe the default values for these parameters in the tool documentation
### Describe actions, not implementations
- model's instructions should describe actions, not when to use specific tools
	- this is to make sure there is no conflict between the actual tool's instruction set and the model's instruction set - can confuse the LLM
	- this is also relevant when the list of available tools can change
- describe what, not how
	- say "create a big to describe the issue" instead of "use the `create_bug` tool to create the issue"
- don't duplicate instructions
	- don't repeat or restate tool instructions or documentation
	- can create an additional dependency between instructions and tool implementation - if tool changes has to update in two places instead of one
- don't dictate workflows
	- describe objective and let the model figure out which tool to use in what order autonomously
- DO explain tool interactions
	- if one tool use will affect another tool, document this and explain it to the model
	- `fetch_web_page` tool may store the retrieved web page in a file - document this somewhere so the agent knows how to access the data
### Publish tasks, not API calls
- tools shouldn't just be wrappers around basic API calls
- tools should instead reflect full end-to-end task workflow that the agent might need to perform
	- this makes it easier for the agent to know which tool to use
- APIs are meant for human developers who are building static workflows, so might have hundreds of possible parameters that influence API output.
- Tools for agents, however, are expected to be used dynamically and on the fly, so if a tool represents a specific task the agent should accomplish, it'll be much more accurate
### Make tools as granular as possible
- SRP - define a clear, single responsibility for each tool
- easier to document the tool and for the agent to know when to use it
- don't generally create multi-tools
	- these are complicated to document and maintain, and can be difficult for agents to use consistently
	- however, if there is a scenario where a common workflow requires many tool calls in a sequence, then it is ok to create that "multi-tool" as long as it is well documented
### Design for concise output
- don't return large responses
	- large tables, dictionaries, files or images can swamp the output context of an LLM
	- frequently stored in an agent's conversation history, can impact subsequent requests
- use external systems
	- if you do need to store large files somewhere, store them in a temp database table so a subsequent tool can retrieve the data directly
### Use validation effectively
- use schema validation for tool inputs and outputs whenever possible
- serve as further documentation of tool's capabilities and function
- provide a run-time check on the tool operation, allows the app itself to validate whether the tool is being called correctly
### Provide descriptive error messages
- the tool's error will also be passed to the LM as output
- so include descriptive error messages as well as future steps that the LM can take
	- ex: "No product data found for product ID XXX. Ask the customer to confirm the product name, and look up the product ID by name to confirm you have the correct ID."
# Model Context Protocol (MCP)

## Core Architectural Components: Hosts, Clients and Servers

**MCP Host**
- application responsible for creating and managing MCP clients
- enforces security policies and content guardrails

**MCP Client**
- the actual component within the host that maintains the connection with the MCP server
- issues commands, receives responses and manages the lifecycle of the communication session with the MCP server

**MCP Server**
- A program that provides a set of capabilities that the server dev wants to make available to AI applicationos
- functions as an adapter for an external tool, data source, or API
- in enterprise contexts, servers are also responsible for security, scalability and governance

## The Communication Layer: JSON-RPC, Transports, and Message Types
- Base Protocol: MCP uses JSON-RPC 2.0 as base message format
- Message Types:
	- Requests: RPC call that expects a response
	- Results: message containing the successful outcome to a corresponding request
	- Errors: A message indicating that a request failed, including a code and description
	- Notifications: A one-way message that cannot be replied to
- Transport Mechanisms
	- stdio
		- only for local communication
		- fast, direct communication where MCP server runs as sub-process of Host application
		- used when tools need to access local resources such as user's filesystem
	- streamable HTTP
		- recommended remote client-server protocol
		- allows stateless servers (standard), with stateful SSE streaming responses (deprecated)

## Key Primitives: Tools and others
- MCP defines several key concepts and entity types
- capabilities offered by server to client include Tools, Resources and Prompts
- capabilities offered by client to server included Sampling, Elicitation, and Roots
- only tools are 99% supported, most client applications don't support the other capabilities

### Tools
- The tool entity is a standardized way for server to describe a function it makes available to clients
- ex: read_file, get_weather, execute_sql, etc.
- MCP servers publish a list of their available tools with descriptions and parameter schemas for agents to discover
- Tool definitions must conform to a JSON schema with
	• name: Unique identifier for the tool
	• title: [OPTIONAL] human-readable name for display purposes
	• description: Human- (and LLM-) readable description of functionality
	• inputSchema: JSON schema defining expected tool parameters
	• outputSchema: [OPTIONAL]: JSON schema defining output structure
	• annotations: [OPTIONAL]: Properties describing tool behavior
- everything in the schema, even the optional things (except annotations), should be included and everything should be carefully worded
- annotations comprise "hints" that are hints on the MCP tool  behavior. These hints are just hints, and aren't promises of exact behavior
	• destructiveHint: May perform destructive updates (default: true).
	• idempotentHint: Calling repeatedly with the same arguments will have no additional
	effect (default: false).
	• openWorldHint: May interact with an "open world" of external entities (default: true).
	• readOnlyHint: Does not modify its environment (default: false)
	• title: Human-readable title for the tool (note that this is not required to agree with the
	title as provided in the tool definition).

Tool results can be whatever the MCP server wants
- can be structured, unstructured
- contain different content types
- can link to other resources on server
- can be returned as single response or stream of responses

Unstructured Content
- can be text, audio, image
- can return specified Resources, which can be a link to a entity stored at another URI, or embedded in tool result
- be cautious of retrieving or using resources returned from an MCP server in this way, should only use Resources from trusted servers

Structured Content
- always returned as a JSON object
- can use outputSchema to validate tool results

Error Handling:
- two standard error reporting mechanisms
	- can return standard JSON-RPC errors for protocol issues such as unknown tools, invalid arguments, etc
	- can return error messages in the tool results by setting the `"isError": true` parameter in the result object
		- usually for errors generated in the operation of the tool itself, like backend API failures, invalid data, business logic errors

### Resources
- server side capability intended to provide contextual data
- examples include log files, configuration data, market statistics, or structured blobs such as PDFs or images
### Prompts
- allows the server to provide reusable prompt examples or templates related to tools and resources
- MCP server can give it's clients a higher-level description of how to use the tools it provides
- there are a lot of security concerns with this
### Sampling
- server requests a client-side LLM call
- gives clients control over security, model type
- very useful for human in the loop frameworks where the user that is going through the client has to approve or reject something
- security risks for potential prompt injections
### Elicitation
- MCP server requests additional user info from the client
- queries the host application dynamically for additional data to complete the tool request
- security concerns once more - servers should not use elicitation to request sensitive information
- users/clients should be able to approve, decline or cancel the request
### Roots
- defines the boundaries of where the servers can operate within the client file system
- very little guardrails in the spec around the behavior of servers with respect to Roots





# References
https://www.youtube.com/watch?v=zTxvGzpfF-g
https://drive.google.com/file/d/1C-HvqgxM7dj4G2kCQLnuMXi1fTpXRdpx/view
https://drive.google.com/file/d/1ENMUDzybOzxnycQQxNh5sE9quRd0s3Sd/view